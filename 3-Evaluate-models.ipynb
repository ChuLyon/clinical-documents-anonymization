{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d60f43-6f19-4404-ad9d-fdbb9cb82365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b598e-f02b-41cc-a3e8-f2aac73b741c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56338a-200f-4974-97ad-037f37e88b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPerplexityHist(data):\n",
    "    perplexities = pd.Series(data)\n",
    "\n",
    "    perplexities.plot.hist(grid=True, density=True, bins=100, edgecolor='black')\n",
    "    plt.axvline(statistics.mean(data), color=\"red\")\n",
    "    plt.title('Perplexity frequency')\n",
    "    plt.xlabel('Perplexities')\n",
    "    plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45c380-b749-47d7-8d8f-0dc2a20f39aa",
   "metadata": {},
   "source": [
    "## before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9a1d3-bbd4-410d-bdbc-f7eb448da8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"path/to/trained/model/not-pretrained_perplexity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jsonfile = json.load(f)\n",
    "    print(str(statistics.mean(jsonfile)), \" +- \", str(statistics.stdev(jsonfile)))\n",
    "    print(str(statistics.median(jsonfile)))\n",
    "    print(str(min(jsonfile)))\n",
    "    print(str(max(jsonfile)))\n",
    "    plotPerplexityHist(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a3f50-535b-4a54-ad7d-01aa14712d43",
   "metadata": {},
   "source": [
    "## After pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76b07a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"path/to/trained/model/pretrained_perplexity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jsonfile = json.load(f)\n",
    "    print(str(statistics.mean(jsonfile)), \" +- \", str(statistics.stdev(jsonfile)))\n",
    "    print(str(statistics.median(jsonfile)))\n",
    "    print(str(min(jsonfile)))\n",
    "    print(str(max(jsonfile)))\n",
    "    plotPerplexityHist(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb67ce",
   "metadata": {},
   "source": [
    "## Test on example texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = {\n",
    "    \"Contrôle 1 an 1/2 après cystoprostatectomie radicale avec Bricker.\":\n",
    "    \"Contrôle 1 an 1/2 après <mask> radicale avec Bricker.\",\n",
    "    \"A ce stade, il existe des ondes lentes diphasiques dans les deux régions frontales intermittentes.\":\n",
    "    \"A ce stade, il existe des ondes lentes <mask> dans les deux régions frontales intermittentes.\",\n",
    "    \"Ordonnance bi-zone Prescriptions relatives au traitement de l'affection de longue durée.\":\n",
    "    \"Ordonnance bi-zone <mask> relatives au traitement de l'affection de longue durée.\",\n",
    "    \"Le contrôle de la fistule huméro-basilique gauche est plutôt bon\":\n",
    "    \"Le contrôle de la <mask> huméro-basilique gauche est plutôt bon\",\n",
    "    \"Déviation du dorsum nasal objectivée et subjective.\":\n",
    "    \"Déviation du <mask> nasal objectivée et subjective.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"not-pretrained\": pipeline(\"fill-mask\", \"path/to/base/model\"),\n",
    "    \"pretrained\": pipeline(\"fill-mask\", \"path/to/trained/model\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in example_dataset:\n",
    "    print(text)\n",
    "    print(example_dataset[text])\n",
    "\n",
    "    for classifier_name in classifiers:\n",
    "        print(classifier_name+\":\")\n",
    "        results = classifiers[classifier_name](example_dataset[text])\n",
    "\n",
    "        for result in results:\n",
    "            print(result[\"token_str\"]+\" (\"+str(round(result[\"score\"], 2))+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7c526-fa6c-4e69-8758-839d09d2adc1",
   "metadata": {},
   "source": [
    "# Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091b8d6-d4ad-4cba-ad6a-0e75fbe6c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_results = {\n",
    "    \"camembert-base\": {\n",
    "        \"not-pretrained-finetuned\": pd.read_csv(\"path/to/finetuned/model/eval-finetuning.csv\", sep=\";\"),\n",
    "        \"pretrained-finetuned\": pd.read_csv(\"path/to/pretrained/and/finetuned/model/eval-finetuning.csv\", sep=\";\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef3952-c657-4888-bbba-81263660f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"label\\t\\tprecision\\t\\trecall\\t\\tf1-score\")\n",
    "for label in [\"CodePostal\", \"Ville\", \"NomPrenom\", \"Voie\", \"IPP\", \"Date\", \"NoDossier\", \"Organisation\", \"SiteWeb\", \"EMail\", \"Localite\", \"Telephone\"]:\n",
    "    print(label, end=\"\")\n",
    "    for score in [\"precision\", \"recall\", \"f1\"]:\n",
    "        key = label+\"-\"+score\n",
    "        for model in cross_results:\n",
    "            for version in cross_results[model]:\n",
    "                if key in cross_results[model][version]:\n",
    "                    result = cross_results[model][version][key]\n",
    "                    print(\n",
    "                        \"\\t\\t\"+\n",
    "                        str(round(statistics.mean(result), 3))+\n",
    "                        \" ± \"+\n",
    "                        str(round(statistics.stdev(result), 3)),\n",
    "                        end=\"\"\n",
    "                     )\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a6827",
   "metadata": {},
   "source": [
    "# Risk of re-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c774480",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_validset = []\n",
    "with open(\"data/data-for-trf-validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"loading json...\")\n",
    "    jsonfile = json.load(f)\n",
    "    print(\"json loaded\")\n",
    "    raw_validset = [datum[\"file.contenu\"] for datum in tqdm(jsonfile)]\n",
    "print(len(raw_validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ff30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer = Whitespace()\n",
    "embedding_dim = 512\n",
    "subtext_size = int(embedding_dim / 2)\n",
    "stride = int(subtext_size / 2)\n",
    "\n",
    "chunked_validset = []\n",
    "for seq in tqdm(raw_validset):\n",
    "    tokenized_seq = pre_tokenizer.pre_tokenize_str(seq)\n",
    "    if len(tokenized_seq) <= subtext_size:\n",
    "        chunked_validset.append(seq)\n",
    "    else:\n",
    "        for i in range(stride, len(tokenized_seq), stride):\n",
    "            sub_tokenized_seq = tokenized_seq[i-stride:i+stride]\n",
    "            sub_seq_start = sub_tokenized_seq[0][1][0]\n",
    "            sub_seq_end = sub_tokenized_seq[-1][1][1]\n",
    "            sub_seq = seq[sub_seq_start:sub_seq_end]\n",
    "            if len(sub_seq) != 0:\n",
    "                chunked_validset.append(sub_seq)\n",
    "print(len(chunked_validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_model = pipeline(\n",
    "    \"token-classification\", \n",
    "    model=\"path/to/finetuned/model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "anon_model.tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_validset = []\n",
    "nbMask = 0\n",
    "for text in tqdm(chunked_validset):\n",
    "    entities = anon_model(text)\n",
    "    anon_text = str(text) #cpy\n",
    "    for entity in entities:\n",
    "        anon_text = anon_text.replace(entity[\"word\"], \"<mask>\", 1)\n",
    "    anon_validset.append({\"text\": anon_text, \"entities\": entities})\n",
    "    nbMask += len(entities)\n",
    "nbMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a36a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "deanon_model = pipeline(\n",
    "    \"fill-mask\", \n",
    "    model=\"path/to/trainedmodel\"\n",
    ")\n",
    "deanon_model.tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ec5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbFound = 0\n",
    "for anon_text in tqdm(anon_validset):\n",
    "    try:\n",
    "        results = deanon_model(anon_text[\"text\"])\n",
    "        if not isinstance(results[0], list):\n",
    "            results = [results]\n",
    "        for i, result in tqdm(enumerate(results)):\n",
    "            word_to_find = anon_text[\"entities\"][i][\"word\"]\n",
    "            for proposal in result:\n",
    "                word_proposed = proposal[\"token_str\"]\n",
    "                if word_to_find.lower() == word_proposed.lower():\n",
    "                    print(\"Word:\", word_to_find, \", Proposed:\", word_proposed)\n",
    "                    nbFound += 1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc58125",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbFound / nbMask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
